{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FinalPipeline.ipynb result report.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Dataset Overview:\n",
    "- The dataset used for analysis is from YouTube comments, containing various features such as 'Text', 'Number of Comments', 'Number of Subscribers', 'Membership Duration', 'Number of Uploads', 'Profanity in UserID', 'Age', and 'Label' (target variable).\n",
    "- After preprocessing, irrelevant columns like 'UserIndex' and 'index' were dropped, and the target column was renamed to 'Label'.\n",
    "\n",
    "### Text Preprocessing:\n",
    "- Text preprocessing included converting text to lowercase, removing stopwords, and keeping only alphanumeric tokens.\n",
    "\n",
    "### Sentiment Analysis:\n",
    "- Sentiment analysis was performed on the 'Text' column using TextBlob, resulting in sentiment polarity scores and labels (Positive, Neutral, Negative).\n",
    "\n",
    "### Word Frequency Visualization:\n",
    "- The top 20 most frequent words in the processed text were visualized using a bar chart.\n",
    "\n",
    "### Feature Engineering:\n",
    "- Feature engineering included counting the occurrence of cursed words in each comment.\n",
    "\n",
    "### Correlation Analysis:\n",
    "- A correlation matrix heatmap was generated to visualize the correlation between numerical features and the target variable. From this we can see that Number of cursed words are highly co-related to Label and number of comments is also  highly co-related to Label and number of cursed words.\n",
    "\n",
    "### Feature Selection:\n",
    "The 'processed_text' column was created as part of text preprocessing steps. Here's why it was used:\n",
    "\n",
    "1. **Text Preprocessing**:\n",
    "   - Text preprocessing is a crucial step in natural language processing (NLP) tasks like sentiment analysis or classification.\n",
    "   - In this pipeline, the 'processed_text' column was created by applying text preprocessing techniques such as converting text to lowercase, removing stopwords, and keeping only alphanumeric tokens.\n",
    "   - Preprocessing helps in standardizing the text data, reducing noise, and making it suitable for analysis.\n",
    "\n",
    "2. **Model Input**:\n",
    "   - In the context of the pipeline, machine learning models are trained to predict cyberbullying based on text data.\n",
    "   - The 'processed_text' column serves as the input feature for these models.\n",
    "   - By using preprocessed text data, the models can focus on extracting meaningful patterns and relationships without being influenced by irrelevant noise or inconsistencies in the raw text.\n",
    "\n",
    "3. **Feature Extraction**:\n",
    "   - Text data needs to be converted into a numerical representation before it can be fed into machine learning models.\n",
    "   - In this pipeline, a TF-IDF (Term Frequency-Inverse Document Frequency) vectorizer is used within the pipeline to convert the preprocessed text data into numerical features.\n",
    "   - TF-IDF vectorization assigns weights to words based on their frequency in each document relative to their frequency across all documents. This helps in capturing the importance of words in distinguishing cyberbullying from non-cyberbullying comments.\n",
    "\n",
    "Overall, using the 'processed_text' column facilitates the integration of text data into the machine learning pipeline by ensuring that it is appropriately preprocessed and ready for model training and analysis.\n",
    "\n",
    "### Model Evaluation:\n",
    "- The dataset was oversampled using RandomOverSampler to handle class imbalance.\n",
    "- Several classification models were trained and evaluated including Logistic Regression, Random Forest, XGBoost, SVM, and Decision Tree.\n",
    "- Evaluation metrics such as accuracy, area under the ROC curve (AUC), confusion matrices, and classification reports were generated for each model.\n",
    "\n",
    "### Model Performance:\n",
    "- Logistic Regression achieved an accuracy of 91.88% with an AUC of 97.13%.\n",
    "- Random Forest outperformed with an accuracy of 99.59% and an AUC of 99.99%.\n",
    "- XGBoost achieved an accuracy of 94.83% with an AUC of 98.84%.\n",
    "- SVM performed well with an accuracy of 99.43% and an AUC of 99.75%.\n",
    "- Decision Tree had an accuracy of 89.75% with an AUC of 90.12%.\n",
    "\n",
    "### Conclusion:\n",
    "- Random Forest and SVM exhibited the highest accuracies and AUC scores, indicating their effectiveness in predicting cyberbullying in YouTube comments.\n",
    "- The presence of cursed words and sentiment polarity were identified as important features in predicting cyberbullying.\n",
    "- Further optimization and fine-tuning of models could potentially enhance performance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
