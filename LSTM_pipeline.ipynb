{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 0.5857\n",
      "Epoch [2/5], Train Loss: 0.1790\n",
      "Epoch [3/5], Train Loss: 0.0504\n",
      "Epoch [4/5], Train Loss: 0.0119\n",
      "Epoch [5/5], Train Loss: 0.0068\n",
      "Accuracy: 0.963084495488105, AUC: 0.9639164983164983\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Custom Tokenizer function using NLTK\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalnum()]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Custom dataset class for text data\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# LSTM Classifier model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, bidirectional, dropout):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "        self.rnn = nn.LSTM(hidden_dim, hidden_dim, num_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1) if self.rnn.bidirectional else hidden[-1, :, :])\n",
    "        return self.fc(hidden.squeeze(0))\n",
    "\n",
    "# Main function to perform NLP pipeline using LSTM model\n",
    "def nlp_pipeline(file_path, target_column='oh_label', text_column='Text'):\n",
    "    # Read the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.drop(['UserIndex', 'index'], axis=1)\n",
    "    df.rename(columns={target_column: 'Label'}, inplace=True)\n",
    "    \n",
    "    # Text Preprocessing\n",
    "    df['processed_text'] = df[text_column].apply(preprocess_text)\n",
    "    \n",
    "    # Split dataset into features and target\n",
    "    X = df['processed_text']\n",
    "    y = df['Label']\n",
    "    \n",
    "    # Random Oversampling\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    X_resampled, y_resampled = oversampler.fit_resample(X.values.reshape(-1, 1), y)\n",
    "    X_resampled = pd.Series(X_resampled.flatten())\n",
    "    y_resampled = pd.Series(y_resampled)\n",
    "    \n",
    "    # Split resampled data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Tokenization\n",
    "    train_data = [tokenize_text(text) for text in X_train]\n",
    "    vocab = build_vocab_from_iterator(train_data, specials=[\"<unk>\", \"<pad>\"])\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    X_train = [[vocab[token] for token in text] for text in train_data]\n",
    "    test_data = [tokenize_text(text) for text in X_test]\n",
    "    X_test = [[vocab[token] for token in text] for text in test_data]\n",
    "    \n",
    "    # Padding sequences\n",
    "    max_len = 100  # Maximum sequence length\n",
    "    X_train = [text[:max_len] + [0] * (max_len - len(text)) for text in X_train]\n",
    "    X_test = [text[:max_len] + [0] * (max_len - len(text)) for text in X_test]\n",
    "    \n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "    \n",
    "    # Define hyperparameters\n",
    "    input_size = len(vocab)\n",
    "    hidden_size = 128\n",
    "    output_size = 2  # Binary classification\n",
    "    num_layers = 2\n",
    "    bidirectional = True\n",
    "    dropout = 0.5\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 5\n",
    "    batch_size = 32\n",
    "    \n",
    "    # Initialize the LSTM model\n",
    "    model = LSTMClassifier(input_size, hidden_size, output_size, num_layers, bidirectional, dropout)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Convert data to DataLoader\n",
    "    train_dataset = TextDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Train the LSTM model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_lstm_model(model, train_loader, criterion, optimizer, device)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}')\n",
    "    \n",
    "    # Evaluate the LSTM model\n",
    "    test_dataset = TextDataset(X_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    accuracy, auc_score = evaluate_lstm_model(model, test_loader, criterion, device)\n",
    "    print(f'Accuracy: {accuracy}, AUC: {auc_score}')\n",
    "\n",
    "# Function to train the LSTM model\n",
    "def train_lstm_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Function to evaluate the LSTM model\n",
    "def evaluate_lstm_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    auc_score = roc_auc_score(true_labels, predictions)\n",
    "    return accuracy, auc_score\n",
    "\n",
    "nlp_pipeline('cyberbullying/youtube_parsed_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicate the performance of the LSTM model on the test dataset after training. Here's an explanation of the provided results:\n",
    "\n",
    "1. **Epoch-wise Training Loss**:\n",
    "   - During training, the loss decreases significantly with each epoch.\n",
    "   - The decreasing trend of the training loss indicates that the model is learning to minimize the difference between predicted and actual labels.\n",
    "\n",
    "2. **Accuracy**:\n",
    "   - The accuracy of approximately 96.3% indicates that the model correctly classified around 96.3% of the instances in the test dataset.\n",
    "   - This metric is calculated by dividing the number of correctly classified instances by the total number of instances.\n",
    "\n",
    "3. **AUC (Area Under the ROC Curve)**:\n",
    "   - AUC is a performance metric that evaluates the model's ability to distinguish between positive and negative classes.\n",
    "   - The AUC value of approximately 0.963 suggests that the model has good discriminatory power, with a high probability of ranking a randomly chosen positive instance higher than a randomly chosen negative instance.\n",
    "   - An AUC value closer to 1 indicates better model performance.\n",
    "\n",
    "Overall, the results indicate that the LSTM model trained effectively and performs well in classifying text data, achieving high accuracy and AUC scores on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Train Loss: 0.5666\n",
      "Epoch [2/5], Train Loss: 0.1612\n",
      "Epoch [3/5], Train Loss: 0.0468\n",
      "Epoch [4/5], Train Loss: 0.0177\n",
      "Epoch [5/5], Train Loss: 0.0058\n",
      "Accuracy: 0.9671862182116489, AUC: 0.9679164983164983\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Custom Tokenizer function using NLTK\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Text preprocessing function\n",
    "def preprocess_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [token.lower() for token in tokens if token.isalnum()]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Custom dataset class for text data\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# LSTM Classifier model\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, bidirectional, dropout):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, hidden_dim)\n",
    "        self.rnn = nn.LSTM(hidden_dim, hidden_dim, num_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * (2 if bidirectional else 1), output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        output, (hidden, cell) = self.rnn(embedded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1) if self.rnn.bidirectional else hidden[-1, :, :])\n",
    "        return self.fc(hidden.squeeze(0))\n",
    "\n",
    "# Main function to perform NLP pipeline using LSTM model\n",
    "def nlp_pipeline(file_path, target_column='oh_label', text_column='Text'):\n",
    "    # Read the dataset\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = df.drop(['UserIndex', 'index'], axis=1)\n",
    "    df.rename(columns={target_column: 'Label'}, inplace=True)\n",
    "    \n",
    "    # Text Preprocessing\n",
    "    df['processed_text'] = df[text_column].apply(preprocess_text)\n",
    "    \n",
    "    # Split dataset into features and target\n",
    "    X = df['processed_text']\n",
    "    y = df['Label']\n",
    "    \n",
    "    # Random Oversampling\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    X_resampled, y_resampled = oversampler.fit_resample(X.values.reshape(-1, 1), y)\n",
    "    X_resampled = pd.Series(X_resampled.flatten())\n",
    "    y_resampled = pd.Series(y_resampled)\n",
    "    \n",
    "    # Split resampled data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Tokenization\n",
    "    train_data = [tokenize_text(text) for text in X_train]\n",
    "    vocab = build_vocab_from_iterator(train_data, specials=[\"<unk>\", \"<pad>\"])\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    X_train = [[vocab[token] for token in text] for text in train_data]\n",
    "    test_data = [tokenize_text(text) for text in X_test]\n",
    "    X_test = [[vocab[token] for token in text] for text in test_data]\n",
    "    \n",
    "    # Padding sequences\n",
    "    max_len = 100  # Maximum sequence length\n",
    "    X_train = [text[:max_len] + [0] * (max_len - len(text)) for text in X_train]\n",
    "    X_test = [text[:max_len] + [0] * (max_len - len(text)) for text in X_test]\n",
    "    \n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
    "    \n",
    "    # Define hyperparameters\n",
    "    input_size = len(vocab)\n",
    "    hidden_size = 128\n",
    "    output_size = 2  # Binary classification\n",
    "    num_layers = 2\n",
    "    bidirectional = True\n",
    "    dropout = 0.5\n",
    "    learning_rate = 0.001\n",
    "    num_epochs = 5\n",
    "    batch_size = 32\n",
    "    \n",
    "    # Initialize the LSTM model\n",
    "    model = LSTMClassifier(input_size, hidden_size, output_size, num_layers, bidirectional, dropout)\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Convert data to DataLoader\n",
    "    train_dataset = TextDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Train the LSTM model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_lstm_model(model, train_loader, criterion, optimizer, device)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}')\n",
    "    \n",
    "    # Evaluate the LSTM model\n",
    "    test_dataset = TextDataset(X_test_tensor, y_test_tensor)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    auc_score = roc_auc_score(true_labels, predictions)\n",
    "    print(f'Accuracy: {accuracy}, AUC: {auc_score}')\n",
    "    \n",
    "    # Plot Confusion Matrix\n",
    "    plot_confusion_matrix(y_test, predictions)\n",
    "\n",
    "# Function to train the LSTM model\n",
    "def train_lstm_model(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', cbar=False)\n",
    "    plt.xlabel('Predicted labels')\n",
    "    plt.ylabel('True labels')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "nlp_pipeline('cyberbullying/youtube_parsed_dataset.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided code performs a natural language processing (NLP) pipeline using an LSTM (Long Short-Term Memory) neural network model to classify text data. Here's a brief description of the main components used in the code:\n",
    "\n",
    "1. **Data Preprocessing**:\n",
    "   - Text data from a CSV file is loaded using pandas.\n",
    "   - The text data is preprocessed using a custom function (`preprocess_text`) which involves tokenization, converting text to lowercase, and removing stop words.\n",
    "   - Random oversampling is applied to balance the dataset using the `RandomOverSampler` from the imbalanced-learn library.\n",
    "\n",
    "2. **Text Tokenization and Padding**:\n",
    "   - The text is tokenized using NLTK's `word_tokenize` function.\n",
    "   - The vocabulary is built using the `build_vocab_from_iterator` function from torchtext, and tokenized text sequences are converted to indices based on this vocabulary.\n",
    "   - Text sequences are padded to ensure uniform length using a maximum sequence length of 100.\n",
    "\n",
    "3. **Model Architecture**:\n",
    "   - An LSTM neural network classifier is defined using PyTorch. The model consists of an embedding layer, an LSTM layer, and a fully connected layer.\n",
    "   - The LSTM model is designed to handle variable-length sequences and is bidirectional with dropout regularization.\n",
    "\n",
    "4. **Model Training and Evaluation**:\n",
    "   - The model is trained on the preprocessed and padded text data using PyTorch's DataLoader for efficient batch processing.\n",
    "   - The training process is performed for a specified number of epochs using the Adam optimizer and cross-entropy loss function.\n",
    "   - The trained model is evaluated on a separate test dataset, and metrics such as accuracy and AUC (Area Under the ROC Curve) are computed using scikit-learn's functions.\n",
    "   - Finally, a confusion matrix is plotted using seaborn and matplotlib to visualize the classification results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
